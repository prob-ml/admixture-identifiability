# %% imports
%load_ext autoreload
%autoreload 2

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

import model
import torch
import math
import matplotlib.pyplot as plt
import numpy as np
import time
import pickle

model.logger.setLevel("INFO")

# %% setup problem

# make the framework of the problem
n = 11
contrib_shape = (3, )*5
N=len(contrib_shape)
contrib_pattern,contrib_pattern_idxs = model.contribution_sparsity_pattern_E0(contrib_shape)
n_params = len(contrib_pattern)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# send contrib pattern to device
contrib_pattern = torch.tensor(contrib_pattern, dtype=torch.int, device=device)

# pick a ground truth
p0 = .1 # probability of off
patterns=[10,16]
secret_v= torch.full((n_params,), .0001, dtype=torch.float64, device=device)
secret_v[0] = p0
print('patterns in truth')
print(f'.   [off] --> {p0}')
print(f'.   [default] --> propto {secret_v[1].item()}')
for p in patterns:
    secret_v[p]=1
    print('.  ',contrib_pattern[p+1].cpu().numpy(), '--> propto ', secret_v[p].item())
secret_v[1:] = (1-p0)*secret_v[1:]/secret_v[1:].sum()
contribution_pmf = model.parameters_to_contribution_pmf(
    secret_v, contrib_shape, contrib_pattern)

# make initial guess
guess_patterns=[17,3,19,9,13,14,15,11,4,16,10]
initial_guess_v= torch.full((n_params,), .01, dtype=torch.float64, device=device)
initial_guess_v[0] = p0
print('\npatterns in initial guess')
print(f'.   [off] --> {initial_guess_v[0].item()}')
print(f'.   [default] --> propto {initial_guess_v[1].item()}')
for p in guess_patterns:
    initial_guess_v[p]= 1.0
    print('.  ',contrib_pattern[p+1].cpu().numpy(), '--> propto', initial_guess_v[p].item())
initial_guess_v[1:] = (1-initial_guess_v[0])*initial_guess_v[1:]/initial_guess_v[1:].sum()

# use ground truth to make observables
# and sanity check
observation_pmf = model.contribution_pmf_to_observation_pmf(contribution_pmf, n)
print("\nobservation_pmf sum",torch.sum(observation_pmf.view(-1), dim=0))
print("observation_pmf shape", observation_pmf.shape)
print("observation_pmf smallest value", observation_pmf.min())

# check loss of the truth, and also time how long it takes to get
print("\ntime checks, and loss of the truth")
for i in range(2):
    params = torch.nn.Parameter(secret_v.clone())
    start_time = time.time()
    loss = model.fit_loss_fn(params,observation_pmf,contrib_shape,contrib_pattern,n)
    print(". loss of truth", loss.item())
    print(". time taken to compute loss", time.time() - start_time)
    loss.backward()
    print(". time taken to compute loss and gradient", time.time() - start_time)

# report number of ops for likelihood evaluation
nops = np.prod(observation_pmf.shape)*len(contrib_pattern)*n
print(f'nops: {nops}={nops/1e6:.2f}M')

# %% do the training
# try to recover contribution_logpmf from observation_logpmf
print('\noptimal loss',model.fit_loss_fn(
    secret_v, observation_pmf, contrib_shape, contrib_pattern, n).item())
print('\ntraining')
start_time = time.time()
params, losses, secret_losses = model.fit_contribution_pmf(
    observation_pmf,
    contrib_shape,
    true_contrib_pmf=contribution_pmf,
    initial_guess=initial_guess_v,
    n_iter=250,
    log_every=10,
    fix_p0=True,
)
print("training time:", time.time() - start_time)

guess_contribution_pmf = model.parameters_to_contribution_pmf(
    params, contrib_shape, contrib_pattern)
guess_observation_pmf = model.contribution_pmf_to_observation_pmf(guess_contribution_pmf, n)

# %% determine gradient at the final guess
params_clone = torch.nn.Parameter(params.clone())
guess_contribution_pmf = model.parameters_to_contribution_pmf(
    params_clone, contrib_shape, contrib_pattern)
guess_observation_pmf = model.contribution_pmf_to_observation_pmf(guess_contribution_pmf, n)
mask = guess_observation_pmf!=0
loss = -torch.sum(observation_pmf[mask] * torch.log(guess_observation_pmf[mask]))
loss.backward()
final_grad = params_clone.grad

# %% calculate loss in convex combination between final guess for contrib_pmf truth about contrib_pmf
mask = observation_pmf != 0
convex_comb_losses = []
alphas = np.r_[0:1:20j]
for i,alpha in enumerate(alphas):
    # when alpha = 0, we use truth
    # when alpha = 1, we use guess
    convex_pmf= alpha * guess_contribution_pmf + (1-alpha) * contribution_pmf
    convex_observation_pmf = model.contribution_pmf_to_observation_pmf(convex_pmf, n)
    loss = -torch.sum(observation_pmf[mask] * torch.log(convex_observation_pmf[mask]))
    convex_comb_losses.append(loss.item())

# %% save the results

with open('local_minima_example.pkl', 'wb') as f:
    pickle.dump(
        {
            'params': params.detach().cpu().numpy(),
            'secret_v': secret_v.detach().cpu().numpy(),
            'observation_pmf': observation_pmf.detach().cpu().numpy(),
            'losses': losses,
            'secret_losses': secret_losses,
            'contrib_pattern': contrib_pattern.detach().cpu().numpy(),
            'initial_guess_v': initial_guess_v.detach().cpu().numpy(),
            'n': n,
            'convex_comb_losses': (alphas,convex_comb_losses),
            'final_grad': final_grad.detach().cpu().numpy(),
        },f)



# %%
