# %% imports
%load_ext autoreload
%autoreload 2

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

import model
import torch
import math
import matplotlib.pyplot as plt
import numpy as np
import time
import pickle

model.logger.setLevel("INFO")

# %% setup problem

# make the framework of the problem
n = 11
contrib_shape = (3, )*5
N=len(contrib_shape)
contrib_pattern,contrib_pattern_idxs = model.contribution_sparsity_pattern_E0(contrib_shape)
n_params = len(contrib_pattern)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# send contrib pattern to device
contrib_pattern = torch.tensor(contrib_pattern, dtype=torch.int, device=device)

# pick a ground truth
patterns=[10,16]
secret_v= torch.full((n_params,), .0001, dtype=torch.float64, device=device)
secret_v[0] = 1
print('patterns in truth')
print(f'.   [off] --> propto {secret_v[0].item()}')
print(f'.   [default] --> propto {secret_v[1].item()}')
for p in patterns:
    secret_v[p]=1
    print('.  ',contrib_pattern[p+1], '--> propto ', secret_v[p])
secret_v = secret_v/secret_v.sum()
contribution_pmf = model.parameters_to_contribution_pmf(
    secret_v, contrib_shape, contrib_pattern)

# make initial guess
guess_patterns=[17,3,19,9,13,14,15,11,4]
initial_guess_v= torch.full((n_params,), .01, dtype=torch.float64, device=device)
initial_guess_v[0] = 1.0
print('\npatterns in initial guess')
print(f'.   [off] --> propto {initial_guess_v[0].item()}')
print(f'.   [default] --> propto {initial_guess_v[1].item()}')
for p in guess_patterns:
    initial_guess_v[p]= 1.0
    print('.  ',contrib_pattern[p+1], '--> propto', initial_guess_v[p])
initial_guess_v = initial_guess_v/initial_guess_v.sum()

# use ground truth to make observables
# and sanity check
observation_pmf = model.contribution_pmf_to_observation_pmf(contribution_pmf, n)
print("\nobservation_pmf sum",torch.sum(observation_pmf.view(-1), dim=0))
print("observation_pmf shape", observation_pmf.shape)
print("observation_pmf smallest value", observation_pmf.min())

# check loss of the truth, and also time how long it takes to get
print("\ntime checks, and loss of the truth")
for i in range(2):
    params = torch.nn.Parameter(secret_v.clone())
    start_time = time.time()
    loss = model.fit_loss_fn(params,observation_pmf,contrib_shape,contrib_pattern,n)
    print(". loss of truth", loss.item())
    print(". time taken to compute loss", time.time() - start_time)
    loss.backward()
    print(". time taken to compute loss and gradient", time.time() - start_time)

# report number of ops for likelihood evaluation
nops = np.prod(observation_pmf.shape)*len(contrib_pattern)*n
print(f'nops: {nops}={nops/1e6:.2f}M')

# %% do the training
# try to recover contribution_logpmf from observation_logpmf
print('\noptimal loss',model.fit_loss_fn(
    secret_v, observation_pmf, contrib_shape, contrib_pattern, n).item())
print('\ntraining')
start_time = time.time()
params, losses, secret_losses = model.fit_contribution_pmf(
    observation_pmf,
    contrib_shape,
    true_contrib_pmf=contribution_pmf,
    initial_guess=initial_guess_v,
    n_iter=1000,
    log_every=10,
)
print("training time:", time.time() - start_time)

guess_contribution_pmf = model.parameters_to_contribution_pmf(
    params, contrib_shape, contrib_pattern)
guess_observation_pmf = model.contribution_pmf_to_observation_pmf(guess_contribution_pmf, n)

# plot losses
plt.gcf().set_size_inches(2,2)
plt.plot(losses[len(losses)//2:])

with open('local_minima_example.pkl', 'wb') as f:
    pickle.dump(
        {
            'params': params.detach().cpu().numpy(),
            'secret_v': secret_v.detach().cpu().numpy(),
            'losses': losses,
            'secret_losses': secret_losses,
            'contrib_pattern': contrib_pattern.detach().cpu().numpy(),
            'n': n,
        })

# %% determine gradient at the final guess
params_clone = torch.nn.Parameter(params.clone())
guess_contribution_pmf = model.parameters_to_contribution_pmf(
    params_clone, contrib_shape, contrib_pattern)
guess_observation_pmf = model.contribution_pmf_to_observation_pmf(guess_contribution_pmf, n)
mask = guess_observation_pmf!=0
loss = -torch.sum(observation_pmf[mask] * torch.log(guess_observation_pmf[mask]))
loss.backward()
grad = params_clone.grad.clone()
mask = params_clone>1e-10
grad = torch.where(mask, grad, torch.zeros_like(grad)) # zero out gradients for zero params
grad[mask] = grad[mask] - grad[mask].mean() # project to zero mean gradients
print(grad)
print('max mag',grad.abs().max())

# %% plot loss in convex combination between final guess for contrib_pmf truth about contrib_pmf
mask = observation_pmf != 0
convex_comb_losses = []
alphas = np.r_[0:1:20j]
for i,alpha in enumerate(alphas):
    # when alpha = 0, we use truth
    # when alpha = 1, we use guess
    convex_pmf= alpha * guess_contribution_pmf + (1-alpha) * contribution_pmf
    convex_observation_pmf = model.contribution_pmf_to_observation_pmf(convex_pmf, n)
    loss = -torch.sum(observation_pmf[mask] * torch.log(convex_observation_pmf[mask]))
    convex_comb_losses.append(loss.item())
plt.gcf().set_size_inches(4,2)
plt.plot(alphas,convex_comb_losses)
plt.ylabel("negative\nlog likelihood")
plt.xticks([0,1],['truth','guess'])
plt.xlabel("one-dimensional family of\nguesses for waveform distribution")
plt.tight_layout()

# check whether our guess is at edge of the convex hull
print('least likely atom',params.min())

# say a bit about the most likely patterns
npp = np.argsort(params.detach().numpy())
print('most likely patterns in the guess')
print(f'.   [off] --> pmf is {params[0].item()}')
for p in npp[-10:][::-1]:
    print('.  ',contrib_pattern[p], f'--> pmf is {params[p].item():.5f}',f"(p# = {p}, true pmf is {secret_v[p].item()})")

# and in the reverse
npp = np.argsort(secret_v.detach().numpy())
print('\nmost likely patterns in the truth')
print(f'.   [off] --> pmf is {params[0].item()}')
for p in npp[-10:][::-1]:
    print('.  ',contrib_pattern[p], f'--> pmf is {params[p].item():.5f}',f"(p# = {p}, true pmf is {secret_v[p].item()})")


# %% say a bit about the most likely patterns
npp = np.argsort(params.detach().numpy())
pvals = params.detach().numpy()[npp[-12:][::-1]]
pvals = np.exp(pvals); pvals = pvals/pvals.sum()
local_contribs = contrib_pattern.detach().numpy()[npp[-12:][::-1]+1]
for p, contrib in zip(pvals, local_contribs):
    print('   ',contrib, f'--> probability is {p:.3f}')

