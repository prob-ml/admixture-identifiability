# %% imports
%load_ext autoreload
%autoreload 2
import model
import torch
import math
import matplotlib.pyplot as plt
import numpy as np
import time

model.logger.setLevel("INFO")

# %% check that contribution_pmf_to_observation_pmf
# seems to be about right

n = 7

pmf = torch.rand(2, 3, 2, 2)
# pmf = torch.zeros(1, 3, 1, 2)
# pmf[0, 0, 0, 0] = .95
# pmf[0, 2, 0, 0] = .05

N=pmf.ndim
pmf = pmf / pmf.sum()

# get theoretical observation pmf
observation_pmf = model.contribution_pmf_to_observation_pmf(pmf, n)

# make a mazillion samples
mc_samples = 5000

observed_sample_counts = torch.zeros_like(observation_pmf)
for i in range(mc_samples):
    U_samples = torch.multinomial(pmf.flatten(), n, replacement=True)
    U_samples_as_vectors = torch.stack(
        torch.unravel_index(U_samples, pmf.shape),
        dim=1,
    )
    X = torch.zeros(n+N-1, dtype=torch.int)
    for j in range(n):
        X[j:j+N] += U_samples_as_vectors[j, :]
    
    X_seen = X[N-1:1-N]

    observed_sample_counts[tuple(X_seen)] += 1

# should be close to 0
print((observed_sample_counts/mc_samples - observation_pmf).abs().max())


# %% example gradienting

# make some ground truth
n = 5
contrib_shape = (2, )*3
N=len(contrib_shape)
contrib_pattern,contrib_pattern_idxs = model.contribution_sparsity_pattern_E0(contrib_shape)
n_params = len(contrib_pattern)
secret_v = torch.randn(n_params)
contribution_pmf = model.parameters_to_contribution_pmf(
    secret_v, contrib_shape, contrib_pattern)

# first let us check that we can convolve pmfs safely
guess_v = torch.rand(n_params)
guess_v = guess_v/guess_v.sum()

guess_v1 = guess_v.clone().requires_grad_()
guess_contribution_pmf = model.parameters_to_contribution_pmf(
    guess_v1, contrib_shape, contrib_pattern)
convolved1 = model.convolve_n_pmfs([
    guess_contribution_pmf[:,    :,    :,    None, None, None, None],
    guess_contribution_pmf[None, :,    :,    :,    None, None, None],
    guess_contribution_pmf[None, None, :,    :,    :,    None, None],
    guess_contribution_pmf[None, None, None, :,    :,    :,    None],
    guess_contribution_pmf[None, None, None, None, :,    :,    :   ],
])
convolved1 = torch.sum(convolved1, dim=(0,1,5,6))

guess_v2 = guess_v.clone().requires_grad_()
guess_contribution_pmf = model.parameters_to_contribution_pmf(
    guess_v2, contrib_shape, contrib_pattern)
convolved2 = model.contribution_pmf_to_observation_pmf(guess_contribution_pmf, n)

mask = convolved1!=0
loss1 = torch.sum(torch.log(convolved1[mask]))
loss1.backward()
grad1 = guess_v1.grad

mask = convolved2!=0
loss2 = torch.sum(torch.log(convolved2[mask]))
loss2.backward()
grad2 = guess_v2.grad

# Confirm that loss1 is close to loss2
assert torch.allclose(loss1, loss2, atol=1e-6), f"loss1 ({loss1}) and loss2 ({loss2}) are not close"

# Confirm that grad1 is close to grad2
assert torch.allclose(grad1, grad2, atol=1e-6), "grad1 and grad2 are not close"


# %% setup example training

# make some ground truth
n = 12
contrib_shape = (3, )*5
N=len(contrib_shape)
contrib_pattern,contrib_pattern_idxs = model.contribution_sparsity_pattern_E0(contrib_shape)
n_params = len(contrib_pattern)

# secret_v = torch.rand(len(contrib_pattern))
# contribution_logpmf = model.parameters_to_contribution_logpmf(
#     secret_v, contrib_shape, contrib_pattern)

patterns=[10,16]
secret_v= torch.full((n_params,), .0001, dtype=torch.float64)
secret_v[0] = 1
print('patterns in truth')
print(f'.   [off] --> propto {secret_v[0].item()}')
print(f'.   [default] --> propto {secret_v[1].item()}')
for p in patterns:
    secret_v[p]=1
    print('.  ',contrib_pattern[p+1], '--> propto ', secret_v[p])
secret_v = secret_v/secret_v.sum()
contribution_pmf = model.parameters_to_contribution_pmf(
    secret_v, contrib_shape, contrib_pattern)

# make initial guess
guess_patterns=[17,3,19,9,13,14,15,11,4]
initial_guess_v= torch.full((n_params,), .01, dtype=torch.float64)
initial_guess_v[0] = 1.0
print('\npatterns in initial guess')
print(f'.   [off] --> propto {initial_guess_v[0].item()}')
print(f'.   [default] --> propto {initial_guess_v[1].item()}')
for p in guess_patterns:
    initial_guess_v[p]= 1.0
    print('.  ',contrib_pattern[p+1], '--> propto', initial_guess_v[p])
initial_guess_v = initial_guess_v/initial_guess_v.sum()

# make initial guess
# rng = np.random.default_rng(0)
# initial_guess_v = torch.from_numpy(
#     rng.normal(
#         loc=0.0,
#         scale=1.0,
#         size=(n_params,)
# ))

# use ground truth to make observables
observation_pmf = model.contribution_pmf_to_observation_pmf(contribution_pmf, n)
print("\nobservation_pmf sum",torch.sum(observation_pmf.view(-1), dim=0))
print("observation_pmf shape", observation_pmf.shape)
print("observation_pmf smallest value", observation_pmf.min())

# check loss of the truth, and time it also
print("\ntime checks, and loss of the truth")
for i in range(2):
    params = torch.nn.Parameter(secret_v.clone())
    start_time = time.time()
    loss = model.fit_loss_fn(params,observation_pmf,contrib_shape,contrib_pattern,n)
    print(". loss of truth", loss.item())
    print(". time taken to compute loss", time.time() - start_time)
    loss.backward()
    print(". time taken to compute loss and gradient", time.time() - start_time)

# report number of ops for likelihood evaluation
nops = np.prod(observation_pmf.shape)*len(contrib_pattern)*n
print(f'nops: {nops}={nops/1e6:.2f}M')

# %% do the training
# try to recover contribution_logpmf from observation_logpmf
print('\noptimal loss',model.fit_loss_fn(
    secret_v, observation_pmf, contrib_shape, contrib_pattern, n).item())
print('\ntraining')
start_time = time.time()
params, losses, secret_losses = model.fit_contribution_pmf(
    observation_pmf,
    contrib_shape,
    true_contrib_pmf=contribution_pmf,
    initial_guess=initial_guess_v,
    n_iter=500,
    log_every=1,
)
print("training time:", time.time() - start_time)

guess_contribution_pmf = model.parameters_to_contribution_pmf(
    params, contrib_shape, contrib_pattern)
guess_observation_pmf = model.contribution_pmf_to_observation_pmf(guess_contribution_pmf, n)

# plot losses
plt.gcf().set_size_inches(2,2)
plt.plot(losses[len(losses)//2:])

# %% determine gradient at the final guess
params_clone = torch.nn.Parameter(params.clone())
guess_contribution_pmf = model.parameters_to_contribution_pmf(
    params_clone, contrib_shape, contrib_pattern)
guess_observation_pmf = model.contribution_pmf_to_observation_pmf(guess_contribution_pmf, n)
mask = guess_observation_pmf!=0
loss = -torch.sum(observation_pmf[mask] * torch.log(guess_observation_pmf[mask]))
loss.backward()
grad = params_clone.grad.clone()
mask = params_clone>1e-10
grad = torch.where(mask, grad, torch.zeros_like(grad)) # zero out gradients for zero params
grad[mask] = grad[mask] - grad[mask].mean() # project to zero mean gradients
print(grad)
print('max mag',grad.abs().max())

# %% do line search in direction of gradient to see if we can do better
loss = model.fit_loss_fn(params,observation_pmf,contrib_shape,contrib_pattern,n)

ls_losses=[]
alphas=np.r_[0:5e-6:20j]
for alpha in alphas:
    ls_losses.append(model.fit_loss_fn(params-grad*alpha,observation_pmf,contrib_shape,contrib_pattern,n).item())
plt.gcf().set_size_inches(2,2)
plt.plot(alphas,ls_losses)
if np.isnan(ls_losses).any():
    print('ls_losses has nans')
print('line search improvement',ls_losses[0]-np.nanmin(ls_losses))

# %% plot loss in convex combination between final guess for contrib_pmf truth about contrib_pmf
mask = observation_pmf != 0
convex_comb_losses = []
alphas = np.r_[0:1:20j]
for i,alpha in enumerate(alphas):
    # when alpha = 0, we use truth
    # when alpha = 1, we use guess
    convex_pmf= alpha * guess_contribution_pmf + (1-alpha) * contribution_pmf
    convex_observation_pmf = model.contribution_pmf_to_observation_pmf(convex_pmf, n)
    loss = -torch.sum(observation_pmf[mask] * torch.log(convex_observation_pmf[mask]))
    convex_comb_losses.append(loss.item())
plt.gcf().set_size_inches(4,2)
plt.plot(alphas,convex_comb_losses)
plt.ylabel("negative\nlog likelihood")
plt.xticks([0,1],['truth','guess'])
plt.xlabel("one-dimensional family of\nguesses for waveform distribution")
plt.tight_layout()

# check whether our guess is at edge of the convex hull
print('least likely atom',params.min())

# say a bit about the most likely patterns
npp = np.argsort(params.detach().numpy())
print('most likely patterns in the guess')
print(f'.   [off] --> pmf is {params[0].item()}')
for p in npp[-10:][::-1]:
    print('.  ',contrib_pattern[p], f'--> pmf is {params[p].item():.5f}',f"(p# = {p}, true pmf is {secret_v[p].item()})")

# and in the reverse
npp = np.argsort(secret_v.detach().numpy())
print('\nmost likely patterns in the truth')
print(f'.   [off] --> pmf is {params[0].item()}')
for p in npp[-10:][::-1]:
    print('.  ',contrib_pattern[p], f'--> pmf is {params[p].item():.5f}',f"(p# = {p}, true pmf is {secret_v[p].item()})")


# %% say a bit about the most likely patterns
npp = np.argsort(params.detach().numpy())
pvals = params.detach().numpy()[npp[-12:][::-1]]
pvals = np.exp(pvals); pvals = pvals/pvals.sum()
local_contribs = contrib_pattern.detach().numpy()[npp[-12:][::-1]+1]
for p, contrib in zip(pvals, local_contribs):
    print('   ',contrib, f'--> probability is {p:.3f}')

