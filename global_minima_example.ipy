# %% imports
%load_ext autoreload
%autoreload 2

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

import model
import torch
import math
import matplotlib.pyplot as plt
import numpy as np
import time
import pickle

model.logger.setLevel("INFO")

# %% setup problem

# make the framework of the problem
n = 9
contrib_shape = (3, )*5
N=len(contrib_shape)
contrib_pattern,contrib_pattern_idxs = model.contribution_sparsity_pattern_E0(contrib_shape)
n_params = len(contrib_pattern)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# send contrib pattern to device
contrib_pattern = contrib_pattern.to(device)

# pick a ground truth
p0 = .9 # probability of off
patterns=[10,16]
secret_v= torch.full((n_params,), .0001, dtype=torch.float64, device=device)
secret_v[0] = p0
print('patterns in truth')
print(f'.   [off] --> {p0}')
print(f'.   [default] --> propto {secret_v[1].item()}')
for p in patterns:
    secret_v[p]=1
    print('.  ',contrib_pattern[p+1].cpu().numpy(), '--> propto ', secret_v[p].item())
secret_v[1:] = (1-p0)*secret_v[1:]/secret_v[1:].sum()
contribution_pmf = model.parameters_to_contribution_pmf(
    secret_v, contrib_shape, contrib_pattern)
observation_pmf = model.contribution_pmf_to_observation_pmf(contribution_pmf, n)

# make initial guess
initial_guess_v= torch.full((n_params,), .01, dtype=torch.float64, device=device)
initial_guess_v[0] = p0
initial_guess_v[1:] = (1-initial_guess_v[0])*initial_guess_v[1:]/initial_guess_v[1:].sum()


# %% do the training
# try to recover contribution_logpmf from observation_logpmf
print('\noptimal loss',model.fit_loss_fn(
    secret_v, observation_pmf, contrib_shape, contrib_pattern, n).item())
print('\ntraining')
start_time = time.time()
params, losses, secret_losses = model.fit_contribution_pmf(
    observation_pmf,
    contrib_shape,
    true_contrib_pmf=contribution_pmf,
    initial_guess=initial_guess_v,
    n_iter=40,
    log_every=10,
    fix_p0=True,
)
print("training time:", time.time() - start_time)

guess_contribution_pmf = model.parameters_to_contribution_pmf(
    params, contrib_shape, contrib_pattern)
guess_observation_pmf = model.contribution_pmf_to_observation_pmf(guess_contribution_pmf, n)

# %% confirm that we nailed it

print("final contribution pmf error",(params[1:]-secret_v[1:]).abs().max().item() / (1-p0))

# %% investigate practical identifiability

# Resample observation_pmf with n=1000 samples
n_samples = 30
flat_obs_pmf = observation_pmf.flatten()
flat_obs_pmf_np = flat_obs_pmf.cpu().numpy()
sampled_idxs = np.random.choice(len(flat_obs_pmf_np), size=n_samples, p=flat_obs_pmf_np)
sampled_counts = np.bincount(sampled_idxs, minlength=len(flat_obs_pmf_np))
resampled_observation_pmf = torch.tensor(sampled_counts / n_samples, dtype=observation_pmf.dtype, device=device)
resampled_observation_pmf = resampled_observation_pmf.reshape(observation_pmf.shape)

print("max sampling error",(resampled_observation_pmf-observation_pmf).abs().max())

# %% do the training again
# try to recover contribution_logpmf from observation_logpmf
print('\nloss of truth',model.fit_loss_fn(
    secret_v, resampled_observation_pmf, contrib_shape, contrib_pattern, n).item())
print('\ntraining')
start_time = time.time()
params, losses, secret_losses = model.fit_contribution_pmf(
    resampled_observation_pmf,
    contrib_shape,
    true_contrib_pmf=contribution_pmf,
    initial_guess=initial_guess_v,
    n_iter=200,
    log_every=25,
    fix_p0=True,
)
print("training time:", time.time() - start_time)

guess_contribution_pmf = model.parameters_to_contribution_pmf(
    params, contrib_shape, contrib_pattern)
guess_observation_pmf = model.contribution_pmf_to_observation_pmf(guess_contribution_pmf, n)

# %% confirm that we didn't exactly nail it

print("final contribution pmf error",(params[1:]-secret_v[1:]).abs().max().item() / (1-p0))
# %%
